{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chainer_repro",
      "provenance": [],
      "authorship_tag": "ABX9TyM+ggfyeF+0OkSS4KyNxzkC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tjsgertsen/Chainer_repro/blob/master/Chainer_repro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFhtiSsKxuzw",
        "colab_type": "text"
      },
      "source": [
        "# Reproduction by using Chainer\n",
        "\n",
        "To reproduce the results of the paper 'Between-class Learning for Image Classification' the code is made available by the authors. However, this code is written in python 2 with a very early version of the Chainer library. Since we had to work on Google Colabs in order to make use of GPU computing this old versions did not work anymore. Therefore, we decided to approach the reproduction in two ways: \n",
        "1. by building our own version in Pytorch (other google colab)\n",
        "2. Update the original code from the authors by making it compatible with python 3 and the newest version of Chainer. To keep the structure of the code intact it was modified offline whereafter is loaded into this Google Colab. Furthermore, an effort is made to change as little as possible since the goal is to reproduce. The (offline) code can be found in the following Github repository: [https://github.com/tjsgertsen/Chainer_repro.git](https://)\n",
        "\n",
        "First, your google drive is mounted. Change `root_path` to your folder where the (offline) code is stored in google drive. You still have to download the CIFAR-10 into a folder named \"datasets\" within the same folder as the code.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAwd3328p7bu",
        "colab_type": "code",
        "outputId": "36430d6c-5e69-4d36-ce9a-69f3ead9eba9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import chainer\n",
        "\n",
        "# mount drive\n",
        "drive.mount('/content/gdrive', force_remount=True) \n",
        "\n",
        "# change working directory\n",
        "root_path='/content/gdrive/My Drive/Cloud Sync/bc_learning_image-master_python3' \n",
        "os.chdir(root_path)\n",
        "check = os.getcwd()\n",
        "print(f'Check: the current working directory is {check}')\n",
        "\n",
        "!python --version\n",
        "print(f'Chainer {chainer.__version__}')\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "Check: the current working directory is /content/gdrive/My Drive/Cloud Sync/bc_learning_image-master_python3\n",
            "Python 3.6.9\n",
            "Chainer 6.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vi6T4jo5SEK",
        "colab_type": "text"
      },
      "source": [
        "# Run Script\n",
        "\n",
        "In order to initialize the code the following python command is triggered. A few examples for variatonal settings are commented. Please construct your own problem that you want to reproduce or run:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iI0UjBNZuLKF",
        "colab_type": "code",
        "outputId": "49610a0e-08f8-44b7-e2f3-9aa47a04e794",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(f'Check chainer version: {chainer.__version__}')\n",
        "%matplotlib inline\n",
        "\n",
        "# command to run with cifar-10 dataset and standard method\n",
        "# !python main.py --dataset cifar10 --netType convnet --nTrials 3 --data /content/gdrive/My\\ Drive/Cloud\\ Sync/bc_learning_image-master_python3/datasets/cifar-10-batches-py\n",
        "\n",
        "# command to run with cifar-10 dataset and BC(+) method\n",
        "!python main.py --dataset cifar10 --netType convnet --nTrials 2 --data /content/gdrive/My\\ Drive/Cloud\\ Sync/bc_learning_image-master_python3/datasets/cifar-10-batches-py --BC --plus \\\n",
        "--save /content/gdrive/My\\ Drive/Cloud\\ Sync/bc_learning_image-master_python3/results\n",
        "\n",
        "# command to run with cifar-100 dataset\n",
        "#!python main.py --dataset cifar100 --netType convnet --data /content/gdrive/My Drive/Cloud Sync/bc_learning_image-master_python3/datasets/cifar-100-python --BC --plus"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Check chainer version: 6.5.0\n",
            "+------------------------------+\n",
            "| CIFAR classification\n",
            "+------------------------------+\n",
            "| dataset  : cifar10\n",
            "| netType  : convnet\n",
            "| learning : BC+\n",
            "| nEpochs  : 250\n",
            "| LRInit   : 0.1\n",
            "| schedule : [0.4, 0.6, 0.8]\n",
            "| warmup   : 0\n",
            "| batchSize: 128\n",
            "+------------------------------+\n",
            "+-- Trial 1 --+\n",
            "\u001b[K| Epoch: 1/250 | Train: LR 0.1  Loss 1.574  top1 73.58 | Val: top1 53.41\n",
            "\u001b[K| Epoch: 2/250 | Train: LR 0.1  Loss 1.316  top1 54.02 | Val: top1 40.44\n",
            "\u001b[K| Epoch: 3/250 | Train: LR 0.1  Loss 1.150  top1 43.42 | Val: top1 30.04\n",
            "\u001b[K| Epoch: 4/250 | Train: LR 0.1  Loss 1.056  top1 37.71 | Val: top1 38.87\n",
            "\u001b[K| Epoch: 5/250 | Train: LR 0.1  Loss 0.993  top1 34.35 | Val: top1 27.74\n",
            "\u001b[K| Epoch: 6/250 | Train: LR 0.1  Loss 0.959  top1 32.65 | Val: top1 25.48\n",
            "\u001b[K| Epoch: 7/250 | Train: LR 0.1  Loss 0.930  top1 31.02 | Val: top1 25.05\n",
            "\u001b[K| Epoch: 8/250 | Train: LR 0.1  Loss 0.908  top1 29.74 | Val: top1 21.30\n",
            "\u001b[K| Epoch: 9/250 | Train: LR 0.1  Loss 0.889  top1 28.73 | Val: top1 19.35\n",
            "\u001b[K| Epoch: 10/250 | Train: LR 0.1  Loss 0.878  top1 28.34 | Val: top1 31.72\n",
            "\u001b[K| Epoch: 11/250 | Train: LR 0.1  Loss 0.874  top1 28.14 | Val: top1 19.33\n",
            "\u001b[K| Epoch: 12/250 | Train: LR 0.1  Loss 0.855  top1 27.00 | Val: top1 22.61\n",
            "\u001b[K| Epoch: 13/250 | Train: LR 0.1  Loss 0.854  top1 27.04 | Val: top1 19.37\n",
            "\u001b[K| Epoch: 14/250 | Train: LR 0.1  Loss 0.851  top1 27.16 | Val: top1 26.92\n",
            "\u001b[K| Epoch: 15/250 | Train: LR 0.1  Loss 0.840  top1 26.62 | Val: top1 19.21\n",
            "\u001b[K| Epoch: 16/250 | Train: LR 0.1  Loss 0.833  top1 26.33 | Val: top1 22.04\n",
            "\u001b[K| Epoch: 17/250 | Train: LR 0.1  Loss 0.838  top1 26.45 | Val: top1 21.87\n",
            "\u001b[K| Epoch: 18/250 | Train: LR 0.1  Loss 0.827  top1 26.14 | Val: top1 23.24\n",
            "\u001b[K| Epoch: 19/250 | Train: LR 0.1  Loss 0.822  top1 25.95 | Val: top1 18.35\n",
            "\u001b[K| Epoch: 20/250 | Train: LR 0.1  Loss 0.822  top1 25.96 | Val: top1 27.97\n",
            "\u001b[K| Epoch: 21/250 | Train: LR 0.1  Loss 0.822  top1 25.98 | Val: top1 21.58\n",
            "\u001b[K| Epoch: 22/250 | Train: LR 0.1  Loss 0.810  top1 25.31 | Val: top1 17.81\n",
            "\u001b[K| Epoch: 23/250 | Train: LR 0.1  Loss 0.816  top1 25.50 | Val: top1 17.41\n",
            "\u001b[K| Epoch: 24/250 | Train: LR 0.1  Loss 0.808  top1 25.26 | Val: top1 21.56\n",
            "\u001b[K| Epoch: 25/250 | Train: LR 0.1  Loss 0.807  top1 25.40 | Val: top1 17.47\n",
            "\u001b[K| Epoch: 26/250 | Train: LR 0.1  Loss 0.801  top1 24.95 | Val: top1 17.14\n",
            "\u001b[K| Epoch: 27/250 | Train: LR 0.1  Loss 0.802  top1 24.92 | Val: top1 17.92\n",
            "\u001b[K| Epoch: 28/250 | Train: LR 0.1  Loss 0.803  top1 25.09 | Val: top1 19.17\n",
            "\u001b[K| Epoch: 29/250 | Train: LR 0.1  Loss 0.798  top1 24.93 | Val: top1 17.03\n",
            "\u001b[K| Epoch: 30/250 | Train: LR 0.1  Loss 0.799  top1 24.97 | Val: top1 16.76\n",
            "\u001b[K| Epoch: 31/250 | Train: LR 0.1  Loss 0.800  top1 24.71 | Val: top1 15.36\n",
            "\u001b[K| Epoch: 32/250 | Train: LR 0.1  Loss 0.792  top1 24.63 | Val: top1 19.47\n",
            "\u001b[K| Epoch: 33/250 | Train: LR 0.1  Loss 0.795  top1 24.83 | Val: top1 18.34\n",
            "\u001b[K| Epoch: 34/250 | Train: LR 0.1  Loss 0.787  top1 24.13 | Val: top1 15.24\n",
            "\u001b[K| Epoch: 35/250 | Train: LR 0.1  Loss 0.792  top1 24.51 | Val: top1 16.91\n",
            "\u001b[K| Epoch: 36/250 | Train: LR 0.1  Loss 0.794  top1 24.54 | Val: top1 15.90\n",
            "\u001b[K| Epoch: 37/250 | Train: LR 0.1  Loss 0.789  top1 24.34 | Val: top1 18.69\n",
            "\u001b[K| Epoch: 38/250 | Train: LR 0.1  Loss 0.796  top1 24.86 | Val: top1 16.47\n",
            "\u001b[K| Epoch: 39/250 | Train: LR 0.1  Loss 0.786  top1 24.26 | Val: top1 17.82\n",
            "\u001b[K| Epoch: 40/250 | Train: LR 0.1  Loss 0.784  top1 24.37 | Val: top1 17.37\n",
            "\u001b[K| Epoch: 41/250 | Train: LR 0.1  Loss 0.789  top1 24.25 | Val: top1 17.08\n",
            "\u001b[K| Epoch: 42/250 | Train: LR 0.1  Loss 0.787  top1 24.58 | Val: top1 16.93\n",
            "\u001b[K| Epoch: 43/250 | Train: LR 0.1  Loss 0.785  top1 24.44 | Val: top1 28.46\n",
            "\u001b[K| Epoch: 44/250 | Train: LR 0.1  Loss 0.783  top1 24.13 | Val: top1 14.85\n",
            "\u001b[K| Epoch: 45/250 | Train: LR 0.1  Loss 0.786  top1 24.37 | Val: top1 16.86\n",
            "\u001b[K| Epoch: 46/250 | Train: LR 0.1  Loss 0.787  top1 24.23 | Val: top1 17.57\n",
            "\u001b[K| Epoch: 47/250 | Train: LR 0.1  Loss 0.783  top1 24.12 | Val: top1 15.09\n",
            "\u001b[K| Epoch: 48/250 | Train: LR 0.1  Loss 0.779  top1 23.97 | Val: top1 21.78\n",
            "\u001b[K| Epoch: 49/250 | Train: LR 0.1  Loss 0.783  top1 24.25 | Val: top1 18.29\n",
            "\u001b[K| Epoch: 50/250 | Train: LR 0.1  Loss 0.780  top1 24.01 | Val: top1 14.94\n",
            "\u001b[K| Epoch: 51/250 | Train: LR 0.1  Loss 0.781  top1 24.08 | Val: top1 17.54\n",
            "\u001b[K| Epoch: 52/250 | Train: LR 0.1  Loss 0.783  top1 24.08 | Val: top1 21.15\n",
            "\u001b[K| Epoch: 53/250 | Train: LR 0.1  Loss 0.780  top1 23.96 | Val: top1 18.90\n",
            "\u001b[K| Epoch: 54/250 | Train: LR 0.1  Loss 0.779  top1 23.91 | Val: top1 19.94\n",
            "\u001b[K| Epoch: 55/250 | Train: LR 0.1  Loss 0.777  top1 23.95 | Val: top1 20.56\n",
            "\u001b[K| Epoch: 56/250 | Train: LR 0.1  Loss 0.776  top1 23.86 | Val: top1 15.64\n",
            "\u001b[K| Epoch: 57/250 | Train: LR 0.1  Loss 0.778  top1 24.09 | Val: top1 15.76\n",
            "\u001b[K| Epoch: 58/250 | Train: LR 0.1  Loss 0.773  top1 23.83 | Val: top1 23.05\n",
            "\u001b[K| Epoch: 59/250 | Train: LR 0.1  Loss 0.783  top1 24.19 | Val: top1 18.79\n",
            "\u001b[K| Epoch: 60/250 | Train: LR 0.1  Loss 0.780  top1 24.07 | Val: top1 13.98\n",
            "\u001b[K| Epoch: 61/250 | Train: LR 0.1  Loss 0.778  top1 23.94 | Val: top1 15.73\n",
            "\u001b[K| Epoch: 62/250 | Train: LR 0.1  Loss 0.777  top1 24.13 | Val: top1 15.52\n",
            "\u001b[K| Epoch: 63/250 | Train: LR 0.1  Loss 0.776  top1 23.79 | Val: top1 16.16\n",
            "\u001b[K| Epoch: 64/250 | Train: LR 0.1  Loss 0.771  top1 23.95 | Val: top1 16.26\n",
            "\u001b[K| Epoch: 65/250 | Train: LR 0.1  Loss 0.775  top1 24.09 | Val: top1 16.09\n",
            "\u001b[K| Epoch: 66/250 | Train: LR 0.1  Loss 0.776  top1 23.97 | Val: top1 18.44\n",
            "\u001b[K| Epoch: 67/250 | Train: LR 0.1  Loss 0.777  top1 24.03 | Val: top1 25.15\n",
            "\u001b[K| Epoch: 68/250 | Train: LR 0.1  Loss 0.773  top1 23.97 | Val: top1 17.65\n",
            "\u001b[K| Epoch: 69/250 | Train: LR 0.1  Loss 0.772  top1 23.46 | Val: top1 14.05\n",
            "\u001b[K| Epoch: 70/250 | Train: LR 0.1  Loss 0.776  top1 24.05 | Val: top1 18.04\n",
            "\u001b[K| Epoch: 71/250 | Train: LR 0.1  Loss 0.773  top1 23.71 | Val: top1 18.02\n",
            "\u001b[K| Epoch: 72/250 | Train: LR 0.1  Loss 0.773  top1 23.52 | Val: top1 16.75\n",
            "\u001b[K| Epoch: 73/250 | Train: LR 0.1  Loss 0.776  top1 23.92 | Val: top1 20.16\n",
            "\u001b[K| Epoch: 74/250 | Train: LR 0.1  Loss 0.770  top1 23.81 | Val: top1 15.66\n",
            "\u001b[K| Epoch: 75/250 | Train: LR 0.1  Loss 0.775  top1 24.05 | Val: top1 17.37\n",
            "\u001b[K| Epoch: 76/250 | Train: LR 0.1  Loss 0.771  top1 23.90 | Val: top1 18.52\n",
            "\u001b[K| Epoch: 77/250 | Train: LR 0.1  Loss 0.774  top1 23.96 | Val: top1 18.16\n",
            "\u001b[K| Epoch: 78/250 | Train: LR 0.1  Loss 0.767  top1 23.51 | Val: top1 19.79\n",
            "\u001b[K| Epoch: 79/250 | Train: LR 0.1  Loss 0.776  top1 23.84 | Val: top1 22.48\n",
            "\u001b[K| Epoch: 80/250 | Train: LR 0.1  Loss 0.774  top1 23.84 | Val: top1 14.33\n",
            "\u001b[K| Epoch: 81/250 | Train: LR 0.1  Loss 0.768  top1 23.41 | Val: top1 17.37\n",
            "\u001b[K| Epoch: 82/250 | Train: LR 0.1  Loss 0.769  top1 23.49 | Val: top1 22.38\n",
            "\u001b[K| Epoch: 83/250 | Train: LR 0.1  Loss 0.773  top1 23.82 | Val: top1 19.41\n",
            "\u001b[K| Epoch: 84/250 | Train: LR 0.1  Loss 0.767  top1 23.80 | Val: top1 16.76\n",
            "\u001b[K| Epoch: 85/250 | Train: LR 0.1  Loss 0.773  top1 23.54 | Val: top1 18.92\n",
            "\u001b[K| Epoch: 86/250 | Train: LR 0.1  Loss 0.768  top1 23.58 | Val: top1 17.52\n",
            "\u001b[K| Epoch: 87/250 | Train: LR 0.1  Loss 0.769  top1 23.68 | Val: top1 15.99\n",
            "\u001b[K| Epoch: 88/250 | Train: LR 0.1  Loss 0.774  top1 23.83 | Val: top1 18.57\n",
            "\u001b[K| Epoch: 89/250 | Train: LR 0.1  Loss 0.768  top1 23.53 | Val: top1 15.11\n",
            "\u001b[K| Epoch: 90/250 | Train: LR 0.1  Loss 0.771  top1 23.76 | Val: top1 18.55\n",
            "\u001b[K| Epoch: 91/250 | Train: LR 0.1  Loss 0.767  top1 23.48 | Val: top1 19.40\n",
            "\u001b[K| Epoch: 92/250 | Train: LR 0.1  Loss 0.771  top1 23.47 | Val: top1 25.20\n",
            "\u001b[K| Epoch: 93/250 | Train: LR 0.1  Loss 0.768  top1 23.46 | Val: top1 18.88\n",
            "\u001b[K| Epoch: 94/250 | Train: LR 0.1  Loss 0.769  top1 23.58 | Val: top1 14.88\n",
            "\u001b[K| Epoch: 95/250 | Train: LR 0.1  Loss 0.764  top1 23.23 | Val: top1 15.94\n",
            "\u001b[K| Epoch: 96/250 | Train: LR 0.1  Loss 0.768  top1 23.64 | Val: top1 19.95\n",
            "\u001b[K| Epoch: 97/250 | Train: LR 0.1  Loss 0.772  top1 23.64 | Val: top1 17.97\n",
            "\u001b[K| Epoch: 98/250 | Train: LR 0.1  Loss 0.768  top1 23.42 | Val: top1 15.37\n",
            "\u001b[K| Epoch: 99/250 | Train: LR 0.1  Loss 0.769  top1 23.76 | Val: top1 16.75\n",
            "\u001b[K| Epoch: 100/250 | Train: LR 0.1  Loss 0.770  top1 23.94 | Val: top1 18.10\n",
            "\u001b[K| Epoch: 101/250 | Train: LR 0.010000000000000002  Loss 0.688  top1 20.15 | Val: top1 8.35\n",
            "\u001b[K| Epoch: 102/250 | Train: LR 0.010000000000000002  Loss 0.631  top1 18.01 | Val: top1 7.89\n",
            "\u001b[K| Epoch: 103/250 | Train: LR 0.010000000000000002  Loss 0.616  top1 17.63 | Val: top1 7.88\n",
            "\u001b[K| Epoch: 104/250 | Train: LR 0.010000000000000002  Loss 0.600  top1 17.12 | Val: top1 7.08\n",
            "\u001b[K| Epoch: 105/250 | Train: LR 0.010000000000000002  Loss 0.592  top1 17.00 | Val: top1 7.18\n",
            "\u001b[K| Epoch: 106/250 | Train: LR 0.010000000000000002  Loss 0.584  top1 16.89 | Val: top1 7.14\n",
            "\u001b[K| Epoch: 107/250 | Train: LR 0.010000000000000002  Loss 0.576  top1 16.52 | Val: top1 7.44\n",
            "\u001b[K| Epoch: 108/250 | Train: LR 0.010000000000000002  Loss 0.571  top1 16.43 | Val: top1 7.27\n",
            "\u001b[K| Epoch: 109/250 | Train: LR 0.010000000000000002  Loss 0.567  top1 16.36 | Val: top1 6.85\n",
            "\u001b[K| Epoch: 110/250 | Train: LR 0.010000000000000002  Loss 0.563  top1 16.37 | Val: top1 7.01\n",
            "\u001b[K| Epoch: 111/250 | Train: LR 0.010000000000000002  Loss 0.558  top1 16.08 | Val: top1 7.07\n",
            "\u001b[K| Epoch: 112/250 | Train: LR 0.010000000000000002  Loss 0.556  top1 15.98 | Val: top1 7.19\n",
            "\u001b[K| Epoch: 113/250 | Train: LR 0.010000000000000002  Loss 0.552  top1 16.08 | Val: top1 7.23\n",
            "\u001b[K| Epoch: 114/250 | Train: LR 0.010000000000000002  Loss 0.552  top1 15.85 | Val: top1 7.44\n",
            "\u001b[K| Epoch: 115/250 | Train: LR 0.010000000000000002  Loss 0.545  top1 15.72 | Val: top1 7.05\n",
            "\u001b[K| Epoch: 116/250 | Train: LR 0.010000000000000002  Loss 0.545  top1 15.90 | Val: top1 7.05\n",
            "\u001b[K| Epoch: 117/250 | Train: LR 0.010000000000000002  Loss 0.544  top1 15.58 | Val: top1 7.78\n",
            "\u001b[K| Epoch: 118/250 | Train: LR 0.010000000000000002  Loss 0.540  top1 15.54 | Val: top1 7.07\n",
            "\u001b[K| Epoch: 119/250 | Train: LR 0.010000000000000002  Loss 0.543  top1 15.62 | Val: top1 7.38\n",
            "\u001b[K| Epoch: 120/250 | Train: LR 0.010000000000000002  Loss 0.541  top1 15.55 | Val: top1 7.20\n",
            "\u001b[K| Epoch: 121/250 | Train: LR 0.010000000000000002  Loss 0.538  top1 15.60 | Val: top1 7.60\n",
            "\u001b[K| Epoch: 122/250 | Train: LR 0.010000000000000002  Loss 0.538  top1 15.67 | Val: top1 7.26\n",
            "\u001b[K| Epoch: 123/250 | Train: LR 0.010000000000000002  Loss 0.535  top1 15.41 | Val: top1 7.17\n",
            "\u001b[K| Epoch: 124/250 | Train: LR 0.010000000000000002  Loss 0.537  top1 15.73 | Val: top1 7.51\n",
            "\u001b[K| Epoch: 125/250 | Train: LR 0.010000000000000002  Loss 0.535  top1 15.46 | Val: top1 7.14\n",
            "\u001b[K| Epoch: 126/250 | Train: LR 0.010000000000000002  Loss 0.534  top1 15.60 | Val: top1 7.71\n",
            "\u001b[K| Epoch: 127/250 | Train: LR 0.010000000000000002  Loss 0.535  top1 15.48 | Val: top1 7.52\n",
            "\u001b[K| Epoch: 128/250 | Train: LR 0.010000000000000002  Loss 0.533  top1 15.35 | Val: top1 7.44\n",
            "\u001b[K| Epoch: 129/250 | Train: LR 0.010000000000000002  Loss 0.531  top1 15.64 | Val: top1 7.29\n",
            "\u001b[K| Epoch: 130/250 | Train: LR 0.010000000000000002  Loss 0.530  top1 15.42 | Val: top1 7.62\n",
            "\u001b[K| Epoch: 131/250 | Train: LR 0.010000000000000002  Loss 0.531  top1 15.45 | Val: top1 7.54\n",
            "\u001b[K| Epoch: 132/250 | Train: LR 0.010000000000000002  Loss 0.530  top1 15.25 | Val: top1 7.75\n",
            "\u001b[K| Epoch: 133/250 | Train: LR 0.010000000000000002  Loss 0.529  top1 15.14 | Val: top1 7.73\n",
            "\u001b[K| Epoch: 134/250 | Train: LR 0.010000000000000002  Loss 0.530  top1 15.35 | Val: top1 8.01\n",
            "\u001b[K| Epoch: 135/250 | Train: LR 0.010000000000000002  Loss 0.528  top1 15.12 | Val: top1 7.31\n",
            "\u001b[K| Epoch: 136/250 | Train: LR 0.010000000000000002  Loss 0.523  top1 15.16 | Val: top1 8.00\n",
            "\u001b[K| Epoch: 137/250 | Train: LR 0.010000000000000002  Loss 0.527  top1 15.41 | Val: top1 7.73\n",
            "\u001b[K| Epoch: 138/250 | Train: LR 0.010000000000000002  Loss 0.526  top1 15.30 | Val: top1 7.74\n",
            "\u001b[K| Epoch: 139/250 | Train: LR 0.010000000000000002  Loss 0.527  top1 15.12 | Val: top1 7.57\n",
            "\u001b[K| Epoch: 140/250 | Train: LR 0.010000000000000002  Loss 0.522  top1 15.08 | Val: top1 8.12\n",
            "\u001b[K| Epoch: 141/250 | Train: LR 0.010000000000000002  Loss 0.524  top1 15.26 | Val: top1 7.52\n",
            "\u001b[K| Epoch: 142/250 | Train: LR 0.010000000000000002  Loss 0.527  top1 14.93 | Val: top1 7.34\n",
            "\u001b[K| Epoch: 143/250 | Train: LR 0.010000000000000002  Loss 0.522  top1 14.97 | Val: top1 7.13\n",
            "\u001b[K| Epoch: 144/250 | Train: LR 0.010000000000000002  Loss 0.523  top1 14.77 | Val: top1 7.50\n",
            "\u001b[K| Epoch: 145/250 | Train: LR 0.010000000000000002  Loss 0.524  top1 15.21 | Val: top1 7.81\n",
            "\u001b[K* Epoch: 146/250 (147/391) | Train: LR 0.010000000000000002 | Time: 1h09m (ETA: 50m15s)"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qxxgvf2G6USB",
        "colab_type": "text"
      },
      "source": [
        "# Results\n",
        "\n",
        "Since the runtime is restricted by Google Colabs we tried to reproduce table 2 of the paper for all learning methods with the CIFAR-10 dataset and an 11-layer CNN. The standard deviation is constructed by 5 trials and 250 epoch per trial, of each learning method. The results that are found are as following:\n",
        "\n",
        "Learning method | Paper results | Reproduction\n",
        "--- | --- | ---\n",
        "Standard | $6.07 \\pm 0.04$ | $6.11 \\pm 0.15$\n",
        "BC | $5.40 \\pm 0.07$ | $5.39 \\pm 0.07$\n",
        "BC+ | $5.22 \\pm 0.04$ | $5.26 \\pm 0.16$"
      ]
    }
  ]
}